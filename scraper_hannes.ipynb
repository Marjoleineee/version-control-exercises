{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Twitter scraper team 9 to extract a dataset with tweets using the terms Black Lives Matter__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\marjo\\anaconda33\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\marjo\\appdata\\local\\temp\\pip-req-build-v1uh74n7\n",
      "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev96+g47fbc2a from git+https://github.com/JustAnotherArchivist/snscrape.git in c:\\users\\marjo\\anaconda33\\lib\\site-packages\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.6.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.9.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from beautifulsoup4->snscrape==0.3.5.dev96+g47fbc2a) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py): started\n",
      "  Building wheel for snscrape (setup.py): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.3.5.dev96+g47fbc2a-py3-none-any.whl size=50496 sha256=b3dda1541faa9d3e45b6e52bf4ee2b714ee833a164060cf45efb658eece5a3f3\n",
      "  Stored in directory: C:\\Users\\marjo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vi0_oprg\\wheels\\92\\42\\87\\33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Requirement already satisfied: pandas in c:\\users\\marjo\\anaconda33\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marjo\\anaconda33\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/MartinBeckUT/TwitterScraper/blob/master/snscrape/python-wrapper/snscrape-python-wrapper.ipynb\n",
    "\n",
    "#Run the pip install command below if you don't already have the library\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "#Run the below command if you don't already have Pandas\n",
    "!pip install pandas\n",
    "\n",
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the data for first months of 2021__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin with 3000 tweets to search in order to see if it works without taking too much time\n",
    "maxTweets = 3000 \n",
    "#Make a list with the variables and words we want to search for\n",
    "searchq = [\"#blm\" , \"#blacklivesmatter\", \"blm\", \"blacklivesmatter\"]\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list1 = [] \n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq,'since:2021-01-01 until:2021-03-17').get_items()):\n",
    "    if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "        break\n",
    "    tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location]) #add all the items we want to subtrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-04 10:13:17+00:00</td>\n",
       "      <td>1367417858779619335</td>\n",
       "      <td>#BLM @wspd5pio What's the status of this inves...</td>\n",
       "      <td>BrodyMendoza10</td>\n",
       "      <td>Portland, OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-04 10:12:57+00:00</td>\n",
       "      <td>1367417772628570115</td>\n",
       "      <td>#BLM #BlackLivesMatter \\nWhite people hate me ...</td>\n",
       "      <td>6Anon6Anon6Anon</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-04 10:11:23+00:00</td>\n",
       "      <td>1367417382306676736</td>\n",
       "      <td>#BlackLivesMatter \\n#BLM\\n\\nWhite people hate ...</td>\n",
       "      <td>6Anon6Anon6Anon</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-04 10:08:43+00:00</td>\n",
       "      <td>1367416710601601026</td>\n",
       "      <td>üåäüò∑#ThursdayThoughts #Wed #LGBTQ üè≥Ô∏è‚Äçüåà #NOH8 #Re...</td>\n",
       "      <td>Rom_TAlan</td>\n",
       "      <td>Ohio, USA, the World.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-04 10:08:34+00:00</td>\n",
       "      <td>1367416671548366849</td>\n",
       "      <td>#BLM https://t.co/3kg08fsEn5</td>\n",
       "      <td>scallypants</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2021-03-02 22:42:22+00:00</td>\n",
       "      <td>1366881596939321347</td>\n",
       "      <td>Normalize Pastors telling men not to objectify...</td>\n",
       "      <td>Lion_Ministries</td>\n",
       "      <td>Washington, DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2021-03-02 22:40:49+00:00</td>\n",
       "      <td>1366881206441230352</td>\n",
       "      <td>I was born in another country, in another time...</td>\n",
       "      <td>ukrefugee</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2021-03-02 22:40:47+00:00</td>\n",
       "      <td>1366881198526500865</td>\n",
       "      <td>@TomiLahren We are unmasking America- The 5 ce...</td>\n",
       "      <td>PacificNormWest</td>\n",
       "      <td>Duwamish Lands (Seattle)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2021-03-02 22:40:21+00:00</td>\n",
       "      <td>1366881087280979968</td>\n",
       "      <td>#blm #FBRs #Resisters\\n#Hispanos #latinos #res...</td>\n",
       "      <td>PENSARYHABLAR</td>\n",
       "      <td>M√©xico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>2021-03-02 22:39:32+00:00</td>\n",
       "      <td>1366880882963996673</td>\n",
       "      <td>Read this thread to see how the end goal of op...</td>\n",
       "      <td>MegSoYung</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3001 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime             Tweet Id  \\\n",
       "0    2021-03-04 10:13:17+00:00  1367417858779619335   \n",
       "1    2021-03-04 10:12:57+00:00  1367417772628570115   \n",
       "2    2021-03-04 10:11:23+00:00  1367417382306676736   \n",
       "3    2021-03-04 10:08:43+00:00  1367416710601601026   \n",
       "4    2021-03-04 10:08:34+00:00  1367416671548366849   \n",
       "...                        ...                  ...   \n",
       "2996 2021-03-02 22:42:22+00:00  1366881596939321347   \n",
       "2997 2021-03-02 22:40:49+00:00  1366881206441230352   \n",
       "2998 2021-03-02 22:40:47+00:00  1366881198526500865   \n",
       "2999 2021-03-02 22:40:21+00:00  1366881087280979968   \n",
       "3000 2021-03-02 22:39:32+00:00  1366880882963996673   \n",
       "\n",
       "                                                   Text         Username  \\\n",
       "0     #BLM @wspd5pio What's the status of this inves...   BrodyMendoza10   \n",
       "1     #BLM #BlackLivesMatter \\nWhite people hate me ...  6Anon6Anon6Anon   \n",
       "2     #BlackLivesMatter \\n#BLM\\n\\nWhite people hate ...  6Anon6Anon6Anon   \n",
       "3     üåäüò∑#ThursdayThoughts #Wed #LGBTQ üè≥Ô∏è‚Äçüåà #NOH8 #Re...        Rom_TAlan   \n",
       "4                          #BLM https://t.co/3kg08fsEn5      scallypants   \n",
       "...                                                 ...              ...   \n",
       "2996  Normalize Pastors telling men not to objectify...  Lion_Ministries   \n",
       "2997  I was born in another country, in another time...        ukrefugee   \n",
       "2998  @TomiLahren We are unmasking America- The 5 ce...  PacificNormWest   \n",
       "2999  #blm #FBRs #Resisters\\n#Hispanos #latinos #res...    PENSARYHABLAR   \n",
       "3000  Read this thread to see how the end goal of op...        MegSoYung   \n",
       "\n",
       "                      Location  \n",
       "0                 Portland, OR  \n",
       "1                               \n",
       "2                               \n",
       "3        Ohio, USA, the World.  \n",
       "4                       London  \n",
       "...                        ...  \n",
       "2996            Washington, DC  \n",
       "2997               Atlanta, GA  \n",
       "2998  Duwamish Lands (Seattle)  \n",
       "2999                    M√©xico  \n",
       "3000              Brooklyn, NY  \n",
       "\n",
       "[3001 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Location']) #give the collumns names and create our Data Frame\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df1 #see if this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df1.to_csv('BLM2021_Dutch_elections15.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the data for the first monts of 2020 to compare__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin with 3000 tweets to search in order to see if it works without taking too much time\n",
    "maxTweets = 3000 \n",
    "#Make a list with the variables and words we want to search for\n",
    "searchq = [\"#blm\" , \"#blacklivesmatter\", \"blm\", \"blacklivesmatter\"]\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = [] \n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq,'since:2020-01-01 until:2020-03-17').get_items()):\n",
    "    if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location]) #add all the items we want to subtrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Location']) #give the collumns names and create our Data Frame\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df2 #see if this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df2.to_csv('BLM2020_Dutch_elections15.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
